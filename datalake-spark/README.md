# Project: Data Lake

## Introduction

Sparkify, a startup company have grown their user base and now wants to build a data lake to better analyse and understanduser behavior and make decisions to make their product better.

They want to design a Data lake using  Cloud Infrastructure and have acquired AWS webservices.


## Project Description

In this project I have applied  the knowledge of Spark and Data Lakes to build and ETL pipeline for a Data Lake hosted on Amazon S3.

All log files and songs data is stored on S3, I have used spark on aws spark to perform etl  and saved the results in s3 bucket again. 
These files can be used further for analyses.



## Project Datasets

- Song data: s3://udacity-dend/song_data
- Log data: s3://udacity-dend/log_data

### Song Dataset
The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.

song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json

And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.

{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}

### Log Dataset
The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.

The log files in the dataset you'll be working with are partitioned by year and month. For example, here are filepaths to two files in this dataset.

song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json


## Project Structure

Project  include following files:

1. etl.py reads data from S3, processes that data using Spark and writes them back to S3

2. dl.cfg contains AWS Credentials

3. README.md provides discussion on your process and decisions

4. data folder contains sample data

5. dev folder contains jupyter notebook and a etl_local.py script to test your etl file on local machine 

## ETL Pipeline

- run the etl.py file from terminal or use spark submit on emr cluster.

- emr cluster can be created using notebook which in this case works properly with all dependencies installed.

- After running the cluster you can check the files in destination by loading them using spark and verify the   data.

- Time taken by this job depends on the size of emr cluster you have selected and various combinations of spark configurations.

- After verification of results , there is a different aws service called AWS GLUE which is a managed service from 
  AWS. Spark is configured really well and your jobs could run much faster (from my personal experience).